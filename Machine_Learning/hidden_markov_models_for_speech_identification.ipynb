{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import random\n",
    "import os                  # dealing with directories\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Language Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the files\n",
    "nums = range(0, 30) \n",
    "path = r\"D:\\Courses\\CS156\\13.2\\symbol\"\n",
    "files_train_A_ = ['\\language-training-langA-%d' % y for y in nums] \n",
    "files_train_A =[path+i for i in files_train_A_]\n",
    "files_train_B_ = ['\\language-training-langB-%d' % y for y in nums] \n",
    "files_train_B =[path+i for i in files_train_B_]\n",
    "files_train_C_ = ['\\language-training-langC-%d' % y for y in nums] \n",
    "files_train_C =[path+i for i in files_train_C_]\n",
    "\n",
    "nums_test = range(0, 10)\n",
    "files_test_ = ['\\language-test-%d' % y for y in nums_test] \n",
    "files_test =[path+i for i in files_test_]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the train and test files:\n",
    "train_A = [pd.read_csv(i).columns[0] for i in files_train_A]\n",
    "train_B = [pd.read_csv(i).columns[0] for i in files_train_B]\n",
    "train_C = [pd.read_csv(i).columns[0] for i in files_train_C]\n",
    "\n",
    "test = [pd.read_csv(i).columns[0] for i in files_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['k', 'A', 'g', 't', 'o', 'e', 'p']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "letters = list(set(train_A[0]))\n",
    "letters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From our discussion in the class, we derived that the values that achieve the maximum likelhiood can be obtained by counting how many letters come after each letter in the sequence(we did that by taking the derivative of the log of the likeihood function to obtain the paramters that maximize the likelihood). I'm going to apply that here to calculate the elements of the transition matrix. I will do the following:\n",
    "- For each letter (let's say letter 'a'), I will count how many another specific letter come after (e.g. letter b) and then I will divide that count by the total number of occurences of letter \"a\" and then put that proportion in the matrix.\n",
    "- I will repeat that for all the letters until I fill all the matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.04176904, 0.31941032, 0.03685504, 0.04176904, 0.24570025,\n",
       "        0.28501229, 0.02948403],\n",
       "       [0.21212121, 0.01767677, 0.23232323, 0.26515152, 0.02020202,\n",
       "        0.03030303, 0.22222222],\n",
       "       [0.0248307 , 0.26410835, 0.02934537, 0.0248307 , 0.32957111,\n",
       "        0.27765237, 0.0496614 ],\n",
       "       [0.23620309, 0.01986755, 0.22737307, 0.23399558, 0.02428256,\n",
       "        0.02869757, 0.22958057],\n",
       "       [0.21634615, 0.01682692, 0.24278846, 0.24759615, 0.01682692,\n",
       "        0.02644231, 0.23317308],\n",
       "       [0.20772947, 0.02657005, 0.24154589, 0.21980676, 0.02898551,\n",
       "        0.01690821, 0.25845411],\n",
       "       [0.01814059, 0.27210884, 0.03628118, 0.04535147, 0.29478458,\n",
       "        0.30612245, 0.02721088]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transition matrix for each language:\n",
    "transition_matrix_A = np.zeros([7, 7])\n",
    "transition_matrix_B = np.zeros([7, 7])\n",
    "transition_matrix_C = np.zeros([7, 7])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for string in train_A: # for each string in the language A\n",
    "    for i, letter in enumerate(string): # for each letter in that string\n",
    "        if i+1 < len(string): # making sure not to exceed the indices len\n",
    "            next_letter = string[i+1] # I check the letter that comes after it\n",
    "            transition_matrix_A[letters.index(letter), letters.index(next_letter)]+=1 # I increase the count by 1 in the\n",
    "                                                            #transition matrix at the position that correspoinds to \"ab\"\n",
    "\n",
    "\n",
    "# I repeat these steps for all other matrices of the other languages:                \n",
    "\n",
    "for string in train_B:\n",
    "    for i, letter in enumerate(string):\n",
    "        if i+1 < len(string):\n",
    "            next_letter = string[i+1]\n",
    "            transition_matrix_B[letters.index(letter), letters.index(next_letter)]+=1\n",
    "            \n",
    "for string in train_C:\n",
    "    for i, letter in enumerate(string):\n",
    "        if i+1 < len(string):\n",
    "            next_letter = string[i+1]\n",
    "            transition_matrix_C[letters.index(letter), letters.index(next_letter)]+=1\n",
    "        \n",
    "# I normalize the matrices to make sure that the rows sum to 1\n",
    "transition_matrix_A =transition_matrix_A/transition_matrix_A.sum(axis=1)[:,None]\n",
    "transition_matrix_B =transition_matrix_B/transition_matrix_B.sum(axis=1)[:,None]\n",
    "transition_matrix_C =transition_matrix_C/transition_matrix_C.sum(axis=1)[:,None]\n",
    "transition_matrix_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'kApoekepAkogAtgAgokAtkAtpegAtpetkAgettgAgApAptpAtkpAAkeppopotkegAtkeApAtpegAkAgAkApAAgAokoAttgokgtpo'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['k', 'A', 'g', 't', 'o', 'e', 'p']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "letters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Again from our discussion in class, I find the inital distribution by counting how many times each letter comes firt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_dist_A_dic = {'g':0, 'A':0, 'e':0, 't':0, 'k':0, \"p\":0, 'o':0}\n",
    "for string in train_A:\n",
    "    for i, letter in enumerate(string):\n",
    "        initial_dist_A_dic[letter]+=1 # if that letter comes first, I increase by 1\n",
    "        break # I stop the for loop after the first count because I only care about who comes first\n",
    "        \n",
    "        \n",
    "# Repeat these steps for all other languages:\n",
    "        \n",
    "initial_dist_B_dic = {'g':0, 'A':0, 'e':0, 't':0, 'k':0, \"p\":0, 'o':0}\n",
    "for string in train_B:\n",
    "    for i, letter in enumerate(string):\n",
    "        initial_dist_B_dic[letter]+=1\n",
    "        break\n",
    "        \n",
    "initial_dist_C_dic = {'g':0, 'A':0, 'e':0, 't':0, 'k':0, \"p\":0, 'o':0}\n",
    "for string in train_C:\n",
    "    for i, letter in enumerate(string):\n",
    "        initial_dist_C_dic[letter]+=1\n",
    "        break\n",
    "        \n",
    "initial_dist_A = [i/sum(initial_dist_A_dic.values()) for i in initial_dist_A_dic.values()]\n",
    "initial_dist_B = [i/sum(initial_dist_B_dic.values()) for i in initial_dist_B_dic.values()]\n",
    "initial_dist_C = [i/sum(initial_dist_C_dic.values()) for i in initial_dist_C_dic.values()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.16666666666666666,\n",
       " 0.06666666666666667,\n",
       " 0.03333333333333333,\n",
       " 0.23333333333333334,\n",
       " 0.2,\n",
       " 0.13333333333333333,\n",
       " 0.16666666666666666]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_dist_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.13295047e-071, 0.00000000e+000, 4.11825526e-115])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_A[0]\n",
    "my_likelihood(train_A[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.109834890619176e-72"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evidence(train_A[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are trying to find the posterior P(language|string). To calculate that, I will use Bayes rule:\n",
    "P(language|string) = P(string |language ) P(language) / P(string)\n",
    "\n",
    "P(string|language ) is the likelihood which is the probability of a string given a certain language. I'm going to calculate this likelihood by multiplying the probability of the first letter in the string (from the initial distribution) by the probability of the second letter given the first letter (from transition matrix) by the probability of the third given the second and so on for all letters in the string, that should correspond to P(string|language)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "posterior distribution for test #0 is [5.25656317e-61 0.00000000e+00 1.00000000e+00]\n",
      "posterior distribution for test #1 is [1.54258086e-68 0.00000000e+00 1.00000000e+00]\n",
      "posterior distribution for test #2 is [1.00000000e+00 0.00000000e+00 7.59413363e-35]\n",
      "posterior distribution for test #3 is [1.34443778e-46 1.00000000e+00 8.64666350e-59]\n",
      "posterior distribution for test #4 is [1.00000000e+00 0.00000000e+00 1.32153626e-46]\n",
      "posterior distribution for test #5 is [1.00000000e+00 0.00000000e+00 3.84247066e-40]\n",
      "posterior distribution for test #6 is [2.20526323e-63 1.00000000e+00 3.37480257e-62]\n",
      "posterior distribution for test #7 is [1.00000000e+00 0.00000000e+00 5.34293923e-42]\n",
      "posterior distribution for test #8 is [6.65951317e-81 0.00000000e+00 1.00000000e+00]\n",
      "posterior distribution for test #9 is [1.00000000e+00 0.00000000e+00 3.58533799e-46]\n"
     ]
    }
   ],
   "source": [
    "def my_likelihood(string):\n",
    "    likelihood = [] # will keep the likelihood for the three languages here\n",
    "    for j in range(3): #for each one of the three languages\n",
    "        likelihood.append(1) \n",
    "        if j ==0: #if language A:\n",
    "            trans_mat =transition_matrix_A\n",
    "            init_dist = initial_dist_A\n",
    "        elif j ==1: #if language B\n",
    "            trans_mat =transition_matrix_B\n",
    "            init_dist = initial_dist_B\n",
    "        else: #if language C\n",
    "            trans_mat =transition_matrix_C\n",
    "            init_dist = initial_dist_C\n",
    "            \n",
    "            \n",
    "        for i, char in enumerate(string): #for each letter in the string\n",
    "            if i ==0: # I get the probability of the first letter from the initial distribution\n",
    "                init = init_dist[letters.index(char)]\n",
    "                prev_char = char\n",
    "                likelihood[-1]*=init #multuplying by it\n",
    "            \n",
    "            else: # for all other letters otherthan the first one:\n",
    "                #multiply the probability of letter given the one before it (from transition matrix)\n",
    "                likelihood[-1]*=trans_mat[letters.index(prev_char), letters.index(char)]\n",
    "                prev_char = char\n",
    "\n",
    "    return np.array(likelihood) #return the likelihood for the three languages saved in a list\n",
    "\n",
    "\n",
    "def evidence(string): # This is P(string) I calculate it by summing P(string|lanugage) over all languages\n",
    "    likels = my_likelihood(string)\n",
    "    return sum(np.array(likels)*(1/3))\n",
    "    \n",
    "\n",
    "priors = 1/3 #this is P(language = A) which is the prior probability we have about the probability of the language\n",
    "# I assume it's uniform so 1/3 for each langauge\n",
    "\n",
    "\n",
    "# finally, I use the above functions to calculate the posterior over the test: Posterior = likelihood*prior/evidence\n",
    "for i, s in enumerate(test):\n",
    "    posterior = my_likelihood(s)* priors/evidence(s)\n",
    "    print(f'posterior distribution for test #{i} is {posterior}')\n",
    "    \n",
    "# Note that the output is the probability of each language in this order: [Language A, Language B, Language C]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gooooAAAAAAAAAkkkkkkooooAAAeppppppgeeeeepAAppeektetttgggogptttttttkppAAAApetAeegggtttteetttttppAAAAA'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Speaker Identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbol = 'eoggeggAeggepgpppoppogopppgoppoopegAAoAAAtAtttooepooppteeeeotpopppeeoepooopopgoooepoepotpoegogggggkeggpogopopeAtAttAoApAtttAggeAgegettttAAAAtoeeggeteoAopopotoktttpoepegpgtgAgAeeppeoooopgeggpAAAAgAtgegogoeepAtAtAAotAAAtttAtkAAAtAAktAAAtttAtAAoAtteeoopoAoAtoAAtAAApgeoeeeeoeeegteoAopeAkopgpeAgetAeeotAttAAeAAktttkAptAetAttAkAAAttAAkAAAttAAAAg'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm going to take samples from dirichlet distribution to initialize the paramters. For the transition matrix, because speakers are more likely to speak for a while, then I will make the diagonal strong for this matrix so that the probability that the speaker who is speaking right now keep speaking for the next time step is higher. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.16940973, 0.35490741, 0.47568285])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intial_distribution  = np.random.dirichlet((1, 1, 1), 1)[0] #from dirichlet\n",
    "intial_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.81304543e-01, 1.43672412e-02, 4.32821603e-03],\n",
       "       [3.15699184e-04, 9.99573801e-01, 1.10499678e-04],\n",
       "       [3.40412340e-04, 3.17103854e-04, 9.99342484e-01]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transition_matrix  = np.array([ np.random.dirichlet((99, 0.5, 0.5), 1)[0],  #strong diagonal\n",
    "                              np.random.dirichlet((0.5, 99, 0.5), 1)[0],\n",
    "                              np.random.dirichlet((0.5, 0.5, 99), 1)[0] ] )\n",
    "transition_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'A', 'e', 'g', 'k', 'o', 'p', 't'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(symbol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['e', 'o', 'k', 'g', 'p', 'A', 't']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "symbols_list = list(set(symbol))\n",
    "symbols_array = np.array(list(symbol))\n",
    "symbols_list # I'm going to use this list to get the indices of the sybmols in the emission matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.05159403, 0.0270908 , 0.22629079],\n",
       "       [0.01170274, 0.04240994, 0.00933219],\n",
       "       [0.13125716, 0.30102701, 0.06624968],\n",
       "       [0.03427456, 0.43275797, 0.12066659],\n",
       "       [0.21278382, 0.00621636, 0.04197065],\n",
       "       [0.11960789, 0.10592758, 0.14859   ],\n",
       "       [0.43877981, 0.08457034, 0.3869001 ]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emission_matrix = np.random.dirichlet((1, 1, 1, 1, 1, 1, 1), 3).transpose() #from dirichlet\n",
    "emission_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The forward-backward algroithm depends on mainly two functions: the forward which calculates alpha and the backward which calculates Beta I will start by writing the forward function. The forward helps in calculate the probability that the hidden markov model will be at a particular hidden state at a particular time step t given a sequence of observed states O. The equation of alpha that the forward algorithm calculates (using equation A.12 from Jurafsky’s book) is:\n",
    "### $\\alpha_t(j) = \\sum_{i=1}^{N}\\alpha_{t-1}(i) * a_{ij} * b_j(O_t)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where here we sum over alpha of all the speakers from speakers i=1 to N and $a_{ij}$ is the transition matrix probability of moving from speaker i to speaker j and $b_{j}(O_t)$ is the emission matrix probability of the symbol at time step t given speaker j. Implementing this recursinve function in python: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(V, a, b, initial_distribution):\n",
    "    alpha = np.zeros((V.shape[0], a.shape[0])) #starting the matrix which will have the dimensions:\n",
    "                                            #(number of observations x number of speakers)\n",
    "    \n",
    "    alpha[0, :] = intial_distribution* b[symbols_list.index(V[0]),:] # P(speaker|start)*P(symbol|speaker)\n",
    " \n",
    "    for t in range(1, V.shape[0]): #loop over each symbol in the observation\n",
    "        for j in range(a.shape[0]): #loop over each speaker of the 3 speakers\n",
    "           \n",
    "            # the recursive function that I explained above:\n",
    "            alpha[t, j] = alpha[t-1].dot(a[:, j]) * b[symbols_list.index(V[t]),j]\n",
    "                                                                                 \n",
    " \n",
    "    return alpha\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, I'm going to define the function backward. Backward algorithm is very similar to the forward algorithm but it's just reversed. The backward algorithm calculates Beta which is basically the probability of seeing the observations from time  t+1 to the end, given that we are in state i at time t. The equation of Beta (according to equation A.15 in Jurafsky’s book) is:\n",
    "\n",
    "### $\\beta_t(i) = \\sum_{j=1}^{N}\\beta_{t+1}(j) * a_{ij} * b_j(O_t+1)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where here we sum over beta of all the speakers from speakers j=1 to N and $a_{ij}$ is the transition matrix probability of moving from speaker i to speaker j and $b_{j}(O_t+1)$ is the emission matrix probability of the symbol at time step t+1 given speaker j. Implementing this recursinve function in python: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(V, a, b):\n",
    "    beta = np.zeros((V.shape[0], a.shape[0])) #initializing a matrix for beta with dimensions \n",
    "                                                ##(number of observations x number of speakers)\n",
    " \n",
    "   \n",
    "    # putting Beta(T) = 1 where T is the index of the last symbol \n",
    "    beta[V.shape[0] - 1] = np.ones((a.shape[0]))\n",
    " \n",
    "    for t in range(V.shape[0] - 2, -1, -1): #for each symbol going backward\n",
    "        for j in range(a.shape[0]): #for each speaker\n",
    "            beta[t, j] = (beta[t + 1] * b[symbols_list.index(V[t + 1]), :]).dot(a[j, :]) # the function I explained above\n",
    " \n",
    "    return beta\n",
    "                          \n",
    "V = symbols_array\n",
    "a = transition_matrix\n",
    "b = emission_matrix\n",
    "intial_distribution  = np.random.dirichlet((1, 1, 1), 1)[0]\n",
    "#backward(V, a, b)         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, as we have the forward and the backward algorithm implmeneted, then wthe Buam Welch algorithm uses them to update the paramters of the model as the following:\n",
    "\n",
    "We first calculate $\\xi$ which is basically the probability of being in state i at time t and state j at time t +1, given the observation sequence and the model, from Jurafsky’s book equation A.22:\n",
    "\n",
    "# $ \\xi_t(i, j) = \\frac{\\alpha_t(i) a_{ij} b_{jkv}(\n",
    "{t+1}) \\beta _{t+1}(j)}{\\sum_{i=1}^{N}\\sum_{j=1}^{N}\\alpha_i(t)a_{ij}b_{jkv}(t+1)\\beta_{j}(t+1)}   $ \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where alpha and beta are the same alpha and beta that we got from the backward and forward algorithm. And $b_j(O_t+1) is the probability of the symbol at time t+1 given speaker j which can be obtained from the emission matrix. And the sum at the denominator  is from speaker j=1 to N where N is the number of speakers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we just need to define the probability of being in state j at time t, which is called $\\gamma_t$ and is given by (according to Jurafsky’s book equation A.27):\n",
    "\n",
    "## $\\gamma$ = $\\sum_{j=1}^{N} \\xi_{ij}(t)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have everything we need to calculate the paramters of the hidden markov model. According to Jurafsky’s book equations A.23 and A.28:\n",
    "\n",
    "# $a_{ij}$ = $\\frac{\\sum_{t=1}^{T-1} \\xi_{ij}(t)}{\\sum_{t=1}^{T-1} \\gamma(t)}$\n",
    "\n",
    "where $a_{ij}$ is the element in the transition matrix that corresponds to the probability of going from speaker i to speaker j.\n",
    "\n",
    "# $b_{jk}$ = $\\frac{\\sum_{t=1}^{T} \\gamma_{j}(t)1(v(t)=k)}{\\sum_{t=1}^{T} \\gamma_j(t)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally the expectation maximization will go as the following:\n",
    "- We first initialize the initial distribution, the transitiona and emission matrices (which we already did)\n",
    "- E-Step: Calculate $\\xi$ and $\\gamma$\n",
    "- M-Step: Use $\\xi$ and $\\gamma$ to update the matrices using the equations above.\n",
    "- Repeat until convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EM(V, a, b, initial_distribution, n_iter):\n",
    "    M = a.shape[0]\n",
    "    T = len(V)\n",
    " \n",
    "    for n in range(n_iter): #iterate for many steps so convergence can happen\n",
    "        \n",
    "        alpha = forward(V, a, b, initial_distribution) #get alpha\n",
    "        beta = backward(V, a, b) #get beta\n",
    "        \n",
    "        xi = np.zeros((M, M, T - 1)) #initiate xi to be used\n",
    "        for t in range(T - 1):\n",
    "            # I calculate the denominator the xi equation that I explained above\n",
    "            # note that I use symbols_list.index() to get the index of the symbol in the emission matrix\n",
    "            denominator = np.dot(np.dot(alpha[t, :].T, a) * b[symbols_list.index(V[t + 1]),:].T, beta[t + 1, :])\n",
    "            for i in range(M):\n",
    "                #calcualte the numerator in the equation I explained above\n",
    "                numerator = alpha[t, i] * a[i, :] * b[symbols_list.index(V[t + 1]),:].T * beta[t + 1, :].T\n",
    "                xi[i, :, t] = numerator / denominator\n",
    " \n",
    "        gamma = np.sum(xi, axis=1)\n",
    "    \n",
    "        #That's the M-step where I update a:    \n",
    "        a = np.sum(xi, 2) / np.sum(gamma, axis=1).reshape((-1, 1))\n",
    " \n",
    "        # Add additional T'th element in gamma because xi's length is T-1\n",
    "        gamma = np.hstack((gamma, np.sum(xi[:, :, T - 2], axis=0).reshape((-1, 1))))\n",
    "\n",
    "        K = b.shape[0]\n",
    "        denominator = np.sum(gamma, axis=1)\n",
    "        for l in range(K):\n",
    "            # M-step where I update b using the equation that I explained above:\n",
    "            \n",
    "            b[l, :] = np.sum(gamma[:, np.array([symbols_list.index(i) for i in V]) == l], axis=1)\n",
    " \n",
    "        b = b/ denominator\n",
    "        \n",
    "        #printing the matrices every 200 steps to check when convergence happens:\n",
    "        \n",
    "        if n % 200 == 0:  \n",
    "            \n",
    "            print('\\n')\n",
    "            print(f\"After {n} steps: \\n\")\n",
    "            print(\"Transition Matrix:\")\n",
    "            print(a)\n",
    "            print('\\n')\n",
    "            print(\"Emission Matrix:\")\n",
    "            print(b)\n",
    "            \n",
    "    \n",
    "        \n",
    "    return a, b\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "After 0 steps: \n",
      "\n",
      "Transition Matrix:\n",
      "[[0.87264275 0.10985335 0.0175039 ]\n",
      " [0.04501999 0.90705949 0.04792052]\n",
      " [0.02089265 0.02053342 0.95857393]]\n",
      "\n",
      "\n",
      "Emission Matrix:\n",
      "[[0.05872577 0.31543941 0.06392172]\n",
      " [0.35396642 0.20418858 0.04426794]\n",
      " [0.01758576 0.01431716 0.04054836]\n",
      " [0.11180875 0.22062873 0.04957525]\n",
      " [0.3129459  0.1726314  0.01530105]\n",
      " [0.03853652 0.03568336 0.47675209]\n",
      " [0.10643087 0.03711137 0.30963359]]\n",
      "\n",
      "\n",
      "After 200 steps: \n",
      "\n",
      "Transition Matrix:\n",
      "[[0.86967105 0.07617466 0.05415428]\n",
      " [0.10118702 0.86271548 0.03609749]\n",
      " [0.02813623 0.04045491 0.93140886]]\n",
      "\n",
      "\n",
      "Emission Matrix:\n",
      "[[1.66156447e-01 3.28418039e-01 1.66164025e-02]\n",
      " [3.66728066e-01 9.12301077e-02 5.74165415e-02]\n",
      " [0.00000000e+00 1.74416709e-02 5.39095689e-02]\n",
      " [7.93806965e-02 3.44504804e-01 5.61251556e-11]\n",
      " [3.60348368e-01 4.04417471e-02 1.77170181e-02]\n",
      " [2.31733582e-08 1.18150196e-01 4.90893034e-01]\n",
      " [2.73863989e-02 5.98134358e-02 3.63447435e-01]]\n",
      "\n",
      "\n",
      "After 400 steps: \n",
      "\n",
      "Transition Matrix:\n",
      "[[0.86966957 0.07617546 0.05415497]\n",
      " [0.10118971 0.86271337 0.03609692]\n",
      " [0.02813676 0.04045454 0.9314087 ]]\n",
      "\n",
      "\n",
      "Emission Matrix:\n",
      "[[1.66156461e-01 3.28419503e-01 1.66164005e-02]\n",
      " [3.66727092e-01 9.12291911e-02 5.74165334e-02]\n",
      " [0.00000000e+00 1.74416643e-02 5.39096427e-02]\n",
      " [7.93817891e-02 3.44505863e-01 5.15347535e-19]\n",
      " [3.60348613e-01 4.04391635e-02 1.77169971e-02]\n",
      " [5.46159193e-13 1.18150525e-01 4.90893203e-01]\n",
      " [2.73860450e-02 5.98140903e-02 3.63447224e-01]]\n",
      "\n",
      "\n",
      "After 600 steps: \n",
      "\n",
      "Transition Matrix:\n",
      "[[0.86966957 0.07617546 0.05415497]\n",
      " [0.10118971 0.86271337 0.03609692]\n",
      " [0.02813676 0.04045454 0.9314087 ]]\n",
      "\n",
      "\n",
      "Emission Matrix:\n",
      "[[1.66156461e-01 3.28419503e-01 1.66164005e-02]\n",
      " [3.66727091e-01 9.12291909e-02 5.74165334e-02]\n",
      " [0.00000000e+00 1.74416643e-02 5.39096427e-02]\n",
      " [7.93817893e-02 3.44505863e-01 4.73218615e-27]\n",
      " [3.60348613e-01 4.04391630e-02 1.77169971e-02]\n",
      " [1.28753019e-17 1.18150525e-01 4.90893203e-01]\n",
      " [2.73860449e-02 5.98140905e-02 3.63447224e-01]]\n",
      "\n",
      "\n",
      "After 800 steps: \n",
      "\n",
      "Transition Matrix:\n",
      "[[0.86966957 0.07617546 0.05415497]\n",
      " [0.10118971 0.86271337 0.03609692]\n",
      " [0.02813676 0.04045454 0.9314087 ]]\n",
      "\n",
      "\n",
      "Emission Matrix:\n",
      "[[1.66156461e-01 3.28419503e-01 1.66164005e-02]\n",
      " [3.66727091e-01 9.12291909e-02 5.74165334e-02]\n",
      " [0.00000000e+00 1.74416643e-02 5.39096427e-02]\n",
      " [7.93817893e-02 3.44505863e-01 4.34533677e-35]\n",
      " [3.60348613e-01 4.04391630e-02 1.77169971e-02]\n",
      " [3.03525801e-22 1.18150525e-01 4.90893203e-01]\n",
      " [2.73860449e-02 5.98140905e-02 3.63447224e-01]]\n"
     ]
    }
   ],
   "source": [
    "transition_matrix  = np.array([ np.random.dirichlet((10, 1, 1), 1)[0],\n",
    "                              np.random.dirichlet((1, 10, 1), 1)[0],\n",
    "                              np.random.dirichlet((1, 1, 10), 1)[0] ] )\n",
    "emission_matrix = np.random.dirichlet((1, 1, 1, 1, 1, 1, 1), 3).transpose()\n",
    "\n",
    "\n",
    "V = symbols_array\n",
    "a = transition_matrix\n",
    "b = emission_matrix\n",
    "initial_distribution  = np.random.dirichlet((1, 1, 1), 1)[0]\n",
    "\n",
    "transition, emission  = EM(V, a, b, initial_distribution, n_iter=900)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From looking at the matrices above, we can see that they converge because at least for the last 400 steps there were no change in their values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = [1, 2, 3]\n",
    "speaker_1, speaker_2, speaker_3 = transition\n",
    "\n",
    "labels = [\"speaker_1 \", \"speaker_2\", \"speaker_3\"]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.stackplot(x, speaker_1, speaker_2, speaker_3, labels=labels)\n",
    "ax.legend(loc='upper left')\n",
    "plt.xticks(x, x)\n",
    "plt.title(\"Stackplot of the probability of each person speaking\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.86966957, 0.07617546, 0.05415497])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speaker_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.10118971, 0.86271337, 0.03609692])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speaker_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.02813676, 0.04045454, 0.9314087 ])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speaker_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refrences\n",
    "- https://web.stanford.edu/~jurafsky/slp3/A.pdf\n",
    "- http://www.adeveloperdiary.com/data-science/machine-learning/introduction-to-hidden-markov-model/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
